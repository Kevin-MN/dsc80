{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "from IPython.display import HTML, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 16 – Parsing, Regular Expressions\n",
    "\n",
    "## DSC 80, Spring 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Discussion 5 (on web scraping) is **today from 7-8:30PM**, and is due (for extra credit) on **Saturday, May 7th at 11:59PM**.\n",
    "- Lab 6 is due on **Monday, May 9th at 11:59PM**.\n",
    "- Project 3 is released, and is due on **Thursday, May 12th at 11:59PM**.\n",
    "    - See [dsc80.com/project3](https://dsc80.com/project3/) for all the details.\n",
    "    - **Try and download your data today!** There is no checkpoint, so you will have to hold yourself accountable.\n",
    "- Midterm Exam grades are released! See [#935](https://campuswire.com/c/G325FA25B/feed/935) for details.\n",
    "- Later this week, expect to see a \"Grade Report\" that contains a summary of your scores on all assignments this quarter along with a slip day counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Example: Scraping the HDSI Faculty page.\n",
    "- Example: Scraping quotes.\n",
    "- String methods.\n",
    "- Regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Scraping the HDSI Faculty page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HDSI Faculty page\n",
    "\n",
    "Let's try and extract a list of HDSI Faculty from https://datascience.ucsd.edu/about/faculty/faculty/.\n",
    "\n",
    "A good first step is to use the \"inspect element\" tool in our web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fac_response = requests.get('https://datascience.ucsd.edu/about/faculty/faculty/')\n",
    "fac_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(fac_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It seems like the relevant `<div>`s for faculty are the ones where the `data-entry-type` attribute is equal to `'individual'`. Let's find all of those using `find_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "divs = soup.find_all('div', attrs={'data-entry-type': 'individual'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"cn-list-row cn-list-item vcard individual faculty lecturers\" data-entry-id=\"229\" data-entry-slug=\"rod-albuyeh\" data-entry-type=\"individual\" id=\"rod-albuyeh\">\n",
       "<div class=\"cn-entry cn-accordion\" id=\"entry-id-2296277514243b34\">\n",
       "<div class=\"cn-left\" style=\"min-width: 215px;\">\n",
       "<span class=\"cn-image-style\"><span style=\"display: block; max-width: 100%; width: 215px\"><img alt=\"Photo of Rod Albuyeh\" class=\"cn-image photo\" height=\"215\" lazyload=\"1\" loading=\"lazy\" sizes=\"100vw\" srcset=\"//datascience.ucsd.edu/wp-content/uploads/connections-images/rod-albuyeh/Rod-Albuyeh-Web-07dd8c651b197a11107f1c858ce1e390.jpg 1x\" title=\"Photo of Rod Albuyeh\" width=\"215\"/></span></span>\n",
       "</div> <!-- end cn-left-->\n",
       "<div class=\"cn-right\">\n",
       "<h3 style=\"border-bottom: #182A48 1px solid; color:#182A48;\"><a href=\"https://datascience.ucsd.edu/about/faculty/faculty/name/rod-albuyeh/\" title=\"Rod Albuyeh\"><span class=\"fn n notranslate\"><span class=\"given-name\">Rod</span> <span class=\"family-name\">Albuyeh</span></span></a>\n",
       "</h3><h4 class=\"title\">Lecturer</h4>\n",
       "<div class=\"cn-excerpt\" id=\"cn-excerpt-2296277514243b34\">\n",
       "\n",
       "\t\t\tAlbuyeh is Principal Data Scientist at Figure and part-time lecturer at the Halıcıoğlu Data Science Institute at UC San Diego.  He received his Ph.D. in Political Science at USC in 2016.  His specialties lie in anomaly detection for tabular time-series data and machine learning systems--with applications in marketing, fraud, and credit risk.  He is also interested in applying enterprise machine learning approaches to solve problems in the social sciences.\n",
       "\n",
       "Research Interests: Machine Learning, Deployment, Scalable Systems<span class=\"cn-link-more\"><a href=\"https://datascience.ucsd.edu/about/faculty/faculty/name/rod-albuyeh/\">Learn More</a></span>\n",
       "</div> <!--end cn-excerpt-->\n",
       "<div class=\"cn-detail cn-hide\" id=\"cn-detail-2296277514243b34\">\n",
       "<div class=\"cn-bio\" id=\"cn-bio-2296277514243b34\"><div class=\"cn-biography\"><p>Albuyeh is Principal Data Scientist at Figure and part-time lecturer at the Halıcıoğlu Data Science Institute at UC San Diego.  He received his Ph.D. in Political Science at USC in 2016.  His specialties lie in anomaly detection for tabular time-series data and machine learning systems–with applications in marketing, fraud, and credit risk.  He is also interested in applying enterprise machine learning approaches to solve problems in the social sciences.</p>\n",
       "<p>Research Interests: Machine Learning, Deployment, Scalable Systems</p>\n",
       "</div>\n",
       "</div>\n",
       "</div> <!--end cn-detail-->\n",
       "</div> <!--end cn-right-->\n",
       "</div> <!-- end cn-entry -->\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within here, we need to extract each faculty member's name. It seems like names are stored in the `title` attribute within an `<a>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://datascience.ucsd.edu/about/faculty/faculty/name/rod-albuyeh/\" title=\"Rod Albuyeh\"><span class=\"fn n notranslate\"><span class=\"given-name\">Rod</span> <span class=\"family-name\">Albuyeh</span></span></a>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs[0].find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rod Albuyeh'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs[0].find('a').get('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract job titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h4 class=\"title\">Lecturer</h4>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs[0].find('h4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lecturer'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs[0].find('h4').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And bios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0].find('div', attrs={'class': 'cn-bio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0].find('div', attrs={'class': 'cn-bio'}).text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create a DataFrame consisting of names and bios for each faculty member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = [div.find('a').get('title') for div in divs]\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [div.find('h4').text if div.find('h4') else '' for div in divs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios = [div.find('div', attrs={'class': 'cn-bio'}).text.strip() for div in divs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty = pd.DataFrame().assign(name=names, title=titles, bio=bios)\n",
    "faculty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty[faculty['title'] == 'Lecturer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we want to get faculty members' pictures? It seems like we should look at the attributes of an `<img>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0].find('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0].find('img').get('srcset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_picture(name):\n",
    "    idx = names.index(name)\n",
    "    url = divs[idx].find('img').get('srcset')\n",
    "    url = 'https://' + url.strip('/').strip(' 1x')\n",
    "    display(Image(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_picture('Suraj Rampure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Scraping quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Scraping quotes\n",
    "\n",
    "Let's scrape quotes from https://quotes.toscrape.com/.\n",
    "\n",
    "<center><img src=\"imgs/quotes2scrape.png\" width=60%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Specifically, let's try to make a DataFrame that looks like the one below:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>quote</th>\n",
    "      <th>author</th>\n",
    "      <th>author_url</th>\n",
    "      <th>tags</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</td>\n",
    "      <td>Albert Einstein</td>\n",
    "      <td>https://quotes.toscrape.com/author/Albert-Einstein</td>\n",
    "      <td>change,deep-thoughts,thinking,world</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>“It is our choices, Harry, that show what we truly are, far more than our abilities.”</td>\n",
    "      <td>J.K. Rowling</td>\n",
    "      <td>https://quotes.toscrape.com/author/J-K-Rowling</td>\n",
    "      <td>abilities,choices</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</td>\n",
    "      <td>Albert Einstein</td>\n",
    "      <td>https://quotes.toscrape.com/author/Albert-Einstein</td>\n",
    "      <td>inspirational,life,live,miracle,miracles</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The plan\n",
    "\n",
    "Eventually, we will create a single function – `quote_df` – which takes in an integer `n` and returns a **DataFrame** with the quotes on the **first `n` pages** of https://quotes.toscrape.com/.\n",
    "\n",
    "To do this, we will define several helper functions:\n",
    "- `download_page(i)`, which downloads a **single page** (page `i`) and returns a `BeautifulSoup` object of the response.\n",
    "- `process_quote(div)`, which takes in a `<div>` tree corresponding to a **single quote** and returns a Series containing all of the relevant information for that quote.\n",
    "- `process_page(divs)`, which takes in a list of `<div>` trees corresponding to a **single page** and returns a DataFrame containing all of the relevant information for all quotes on that page.\n",
    "\n",
    "Key principle: some of our helper functions will make **requests**, and others will **parse**, but none will do both! \n",
    "- Easier to debug and catch errors.\n",
    "- Avoids unnecessary requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: f-strings in Python\n",
    "\n",
    "- f-strings in Python provide a convenient way to format strings.\n",
    "- To create an f-string, create a string with the character `f` **right before** the opening quote. Then, anything in the subsequent string that is inside `{curly brackets}` will be evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'2 + 3 = {2 + 3}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_greeting(name):\n",
    "    return f\"Hi {name}! 👋 Your name has {len(name)} characters, the first of which is {name[0]}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_greeting('Billy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Downloading a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def download_page(i):\n",
    "    url = f'https://quotes.toscrape.com/page/{i}'\n",
    "    request = requests.get(url)\n",
    "    return bs4.BeautifulSoup(request.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `quote_df`, we will call `download_page` repeatedly – once for `i=1`, once for `i=2`, ..., `i = n`. For now, we will work with just page 5 (chosen arbitrarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = download_page(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parsing a single page\n",
    "\n",
    "Let's look at the page's source code (via \"inspect element\") to find where the quotes in the page are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all('div', attrs={'class': 'quote'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this `<div>`, we can extract the quote, author name, author's URL, and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0].find('span', attrs={'class': 'text'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0].find('small', attrs={'class': 'author'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0].find('a').get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs[0].find('meta', attrs={'class': 'keywords'}).get('content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's write an intermediate function, `process_quote`, which takes in a `<div>` corresponding to a single quote and returns a **Series** containing the quote's information.\n",
    "\n",
    "Note that this approach is different than the approach taken in the HDSI Faculty page example – there, we created each column of our final DataFrame separately, while here we are creating one **row** of our final DataFrame at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_quote(div):\n",
    "    quote = div.find('span', attrs={'class': 'text'}).text\n",
    "    author = div.find('small', attrs={'class': 'author'}).text\n",
    "    author_url = 'https://quotes.toscrape.com' + div.find('a').get('href')\n",
    "    tags = div.find('meta', attrs={'class': 'keywords'}).get('content')\n",
    "    \n",
    "    return pd.Series({'quote': quote, 'author': author, 'author_url': author_url, 'tags': tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_quote(divs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can write a function that takes in a list of `<div>`s, calls the above function on each `<div>` in the list, and returns a **DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(divs):\n",
    "    return pd.DataFrame([process_quote(div) for div in divs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_page(divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_df(n):\n",
    "    '''Returns a DataFrame containing the quotes on the first n pages of https://quotes.toscrape.com/.'''\n",
    "    dfs = []\n",
    "    for i in range(1, n + 1):\n",
    "        # Download page n and create a BeautifulSoup object\n",
    "        soup = download_page(i)\n",
    "        \n",
    "        # Create DataFrame using the information in that page\n",
    "        divs = soup.find_all('div', attrs={'class': 'quote'})\n",
    "        df = process_page(divs)\n",
    "        \n",
    "        # Append DataFrame to dfs\n",
    "        dfs.append(df)\n",
    "        \n",
    "    # Stitch all DataFrames together\n",
    "    return pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_three_pages = quote_df(3)\n",
    "first_three_pages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements in the `'tags'` column are all strings, but they look like lists. This is not ideal, as we will see shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An extension\n",
    "\n",
    "We could:\n",
    "- Request information about each of the **authors** in the DataFrame.\n",
    "    - See https://quotes.toscrape.com/author/Albert-Einstein/ for an example.\n",
    "- Create a DataFrame of author information.\n",
    "- Merge that DataFrame with `first_three_pages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(first_three_pages['author_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einstein = bs4.BeautifulSoup(requests.get('https://quotes.toscrape.com/author/Albert-Einstein').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einstein.find('div', attrs={'class': 'author-description'}).text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key takeaways\n",
    "\n",
    "* Make as few requests as possible.\n",
    "* Create a request and parsing plan **beforehand**.\n",
    "* Create your output schema **beforehand**.\n",
    "* Make requests and parse in **separate functions**!\n",
    "* See Lab 6, Question 2 for a related example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested vs. flat data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nested vs. flat data formats\n",
    "\n",
    "- **Nested** data formats, like HTML, JSON, and XML, allow us to represent hierarchical relationships between variables.\n",
    "\n",
    "* **Flat** (i.e. tabular) data formats, like CSV, do not.\n",
    "\n",
    "<center><img src=\"imgs/hierarchy.png\" width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Scraping quotes, again\n",
    "\n",
    "- Suppose we obtained the quotes data via an API and saved it to the file `data/quotes2scrape.json`.\n",
    "- `quotes2scrape.json` is a **JSON records** file; each line is a valid JSON object, **but the entire document is not**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join('data', 'quotes2scrape.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for a single quote, we have keys for `'auth_url'`, `'quote_auth'`, `'quote_text'`, `'bio'`, `'dob'`, and `'tags'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since each line is a separate JSON object, let's read in each line one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [json.loads(x) for x in open(os.path.join('data', 'quotes2scrape.json'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the result to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(L)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What data type is the `'tags'` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save `df` to a CSV and read it back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_again = pd.read_csv('out.csv')\n",
    "df_again.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What data type is the `'tags'` column now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_again['tags'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-hot encoding\n",
    "\n",
    "- So that we don't have to deal with lists within Series, we can **flatten** lists of tags so that there is **one column per tag**.\n",
    "    - For example, consider the tag `'inspirational'`.\n",
    "    - If a quote has a 1 in the `'inspirational'` column, it **was** tagged `'inspirational'`.\n",
    "    - If a quote has a 0 in the `'inspirational'` column, it **was not** tagged `'inspirational'`.\n",
    "- This process – of converting categorical variables into columns of 1s and 0s – is called **one-hot encoding**. We will revisit it in a few weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_tags = np.unique(df['tags'].sum())\n",
    "distinct_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that takes in the list of tags (`taglist`) for a given quote and returns the one-hot-encoded sequence of 1s and 0s for that quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tags(taglist):\n",
    "    return pd.Series({k:1 for k in taglist}, dtype=float)\n",
    "\n",
    "tags = df['tags'].apply(flatten_tags).fillna(0).astype(int)\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's combine this one-hot-encoded DataFrame with `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df, tags], axis=1).drop(columns='tags')\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want all quotes tagged `'inspiration'`, we can simply query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[df_full['inspirational'] == 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this DataFrame representation of the response JSON takes up much more space than the original JSON. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## String methods, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transitioning\n",
    "\n",
    "<center><img src=\"imgs/DSLC.png\" width=\"30%\"></center>\n",
    "\n",
    "- We've spent a lot of time at the \"Find and Clean Data\" stage.\n",
    "- Today, we're going to start working with text data.\n",
    "    - First, it will be in the context of cleaning data.\n",
    "    - Starting next week, it will be in the context of modeling and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Joining on text\n",
    "\n",
    "Consider the following two DataFrames (see [this presentation](https://docs.google.com/presentation/d/1xQsqa7e3xDZ9nBiekbSBOecwvQm8pSVGa-FBoV6aJ7E/edit#slide=id.g11197671c7e_0_813)) for inspiration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_csv(os.path.join('data', 'codes.csv'))\n",
    "programs = pd.read_csv(os.path.join('data', 'programs.csv'))\n",
    "\n",
    "display(codes)\n",
    "display(programs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we try to merge the two DataFrames on `'department'`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes.merge(programs, on='department')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### String canonicalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One solution is to **canonicalize** both `'department'` columns, so that there is just a single way to format each department's name **in both DataFrames**. \n",
    "- We can do this by implementing a `canonicalize_department` function, which takes in a department's name as a string and reformats it.\n",
    "- `canonicalize_department` should:\n",
    "    - Fix cases (upper vs. lower).\n",
    "    - Standardize variants of words – e.g. `'eng.'` vs `'engineering'`.\n",
    "    - Fix punctuation – e.g. `'&'` vs. `'and'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(codes)\n",
    "display(programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def canonicalize_department(d):\n",
    "    return (d\n",
    "           .lower()\n",
    "           .replace('sci.', 'science')\n",
    "           .replace('stud.', 'studies')\n",
    "           .replace('eng.', 'engineering')\n",
    "           .replace('&', 'and')\n",
    "           .replace('(', '- ')\n",
    "           .replace(')', '')\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes['department_clean'] = codes['department'].apply(canonicalize_department)\n",
    "programs['department_clean'] = programs['department'].apply(canonicalize_department)\n",
    "\n",
    "display(codes)\n",
    "display(programs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can join `codes` with `programs` on `'department_clean'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes.merge(programs, on='department_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reflection\n",
    "\n",
    "The process of **string canonicalization** is very brittle. \n",
    "- `canonicalize_department` was hyper-specific to the four department names we had access to. \n",
    "- We don't know if it'll work for other departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The limitations of string methods\n",
    "\n",
    "How can we extract the date and time from the following **log** string, using just Python string methods?\n",
    "\n",
    "```\n",
    "132.249.20.188 - - [05/May/2022:14:26:15 -0800] \"GET /my/home/ HTTP/1.1\" 200 2585\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parsing log strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s = '''132.249.20.188 - - [05/May/2022:14:26:15 -0800] \"GET /my/home/ HTTP/1.1\" 200 2585'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_date = s.split('[')[1].split(']')[0]\n",
    "full_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day, month, rest = full_date.split('/')\n",
    "day, month, rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, hour, minute, second = rest.split(':')\n",
    "second = second[:2]\n",
    "year, hour, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, day, month, hour, minute, second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(full_date[:-6], format='%d/%b/%Y:%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That was annoying! Let's see if there's a better way to extract the same information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This works...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall('\\[(\\d)+\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+)\\s.*\\]', s)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>🤔🤯</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regular expressions\n",
    "\n",
    "- A regular expression, or **regex** for short, is a sequence of characters used to **match patterns in strings**.\n",
    "    - For example, `[1-9][0-9]{2}-[0-9]{3}-[0-9]{4}` matches US phone numbers of the form `'XXX-XXX-XXXX'`.\n",
    "- They are very powerful and widely used.\n",
    "- However, they are quite difficult to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [regex101.com](https://regex101.com)\n",
    "\n",
    "- Next class, we will learn how to use regular expressions in Python using the `re` package.\n",
    "- However, when crafting regular expressions, it is helpful to work in an environment that provides syntax highlighting and details.\n",
    "- **[regex101.com](https://regex101.com) does exactly that – use it!**\n",
    "    - [This link](https://regex101.com/r/ESor65/1) will bring you to the phone number example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regex building blocks 🧱\n",
    "\n",
    "The four main building blocks for all regexes are shown below ([table source](https://www.cs.princeton.edu/courses/archive/spring17/cos226/lectures/54RegularExpressions.pdf), [inspiration](https://docs.google.com/presentation/d/1xQsqa7e3xDZ9nBiekbSBOecwvQm8pSVGa-FBoV6aJ7E/edit#slide=id.g11197671c7e_0_919)).\n",
    "\n",
    "| operation | order of op. | example | matches ✅ | does not match ❌ |\n",
    "|:--- |:---|:---|:---|:---|\n",
    "| <span style='color:purple'><b>concatenation</b></span> | 3 | `AABAAB` | AABAAB | every other string |\n",
    "| <span style='color:purple'><b>or</b></span> | 4 | `AA\\|BAAB` | AA, BAAB | every other string |\n",
    "| <span style='color:purple'><b>closure</b><br>(zero or more)</span> | 2 | `AB*A` | AA, ABBBBBBA | AB, ABABA |\n",
    "| <span style='color:purple'><b>parentheses</b></span> | 1 | `A(A\\|B)AAB` <hr style=\"height:1px\"> `(AB)*A` | AAAAB, ABAAB<hr style=\"height:1px\">A, ABABABABA | every other string<hr style=\"height:1px\">AA, ABBA |\n",
    "\n",
    "Note that `|`, `(`, `)`, and `*` are **special characters**, not literals. They manipulate the characters around them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`AB*A` matches strings with an `'A'`, followed by zero or more `'B'`s, and then an `'A'`. \n",
    "\n",
    "✅ `'AA'`, `'ABA'`, `'ABBBBBBBBBBBBBBA'`<br>\n",
    "❌ `'AB'`, `'ABAB'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`(AB)*A` matches strings with zero or more `'AB'`s, followed by an `'A'`.\n",
    "\n",
    "✅ `'A'`, `'ABA'`, `'ABABABABA'`<br>\n",
    "❌ `'AA'`, `'ABBBBBBBA'`, `'ABAB'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1\n",
    "\n",
    "Write a regular expression that matches `'billy'`, `'billlly'`, `'billlllly'`, etc.\n",
    "- First, think about how to match strings with any even number of `'l'`s, including zero `'l'`s (i.e. `'biy'`).\n",
    "- Then, think about how to match only strings with a **positive even** number of `'l'`s.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    ✅ Click here to see the answer <b>after</b> you've tried it yourself at <a href='https://regex101.com'>regex101.com</a>.\n",
    "</summary>\n",
    "<code>bi(ll)*y</code> will match any even number of <code>'l'</code>s, including 0.\n",
    "    \n",
    "To match only a positive even number of <code>'l'</code>s, we'd need to first \"fix into place\" two <code>'l'</code>s, and then follow that up with zero or more pairs of <code>'l'</code>s. This specifies the regular expression <code>bill(ll)*y</code>.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2\n",
    "\n",
    "Write a regular expression that matches `'billy'`, `'billlly'`, `'biggy'`, `'biggggy'`, etc.\n",
    "\n",
    "Specifically, it should match any string with a **positive even** number of `'l'`s in the middle, or a **positive even** number of `'g'`s in the middle.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "    ✅ Click here to see the answer <b>after</b> you've tried it yourself at <a href='https://regex101.com'>regex101.com</a>.\n",
    "</summary>\n",
    "\n",
    "Possible answers: `bi(ll(ll)*|gg(gg)*)y` or `bill(ll)*y|bigg(gg)*y`.\n",
    " \n",
    "<br>\n",
    "\n",
    "Note, `bill(ll)*|gg(gg)*y` is <b>not</b> a valid answer! This is because \"concatenation\" comes before \"or\" in the order of operations. This regular expression would match strings that match `bill(ll)*`, like `'billll'`, OR strings that match `gg(gg)*y`, like `'ggy'`.\n",
    "\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- When writing scraping code:\n",
    "     - Use \"inspect element\" to identify the names of tags and attributes that are relevant to the information you want to extract.\n",
    "     - Separate your logic for making requests and for parsing.\n",
    "- Regular expressions allow us to match patterns in strings.\n",
    "- **Next time:** More regex syntax. Using regex in Python.\n",
    "    - You **don't** need to memorize syntax, you just need to know what is possible.\n",
    "    - We will look at some \"cheat sheets\" next class.\n",
    "- **For fun (and practice):** Play [Regex Golf](https://alf.nu/RegexGolf?world=regex&level=r00)!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
